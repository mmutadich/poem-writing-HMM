{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKq-wWnDyjDf"
      },
      "source": [
        "# Shakespearean Sonnet Generation Using Hidden Markov Model \n By Mia Mutadich, Lana Lubecke, Jena Alsup, Ava Penn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH_uldS-yjDh"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR_UT6qLl0BG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PVeW5DOyjDi"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url_dict = {\n",
        "    'shakespeare.txt': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/shakespeare.txt',\n",
        "    'spenser.txt': 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/spenser.txt',\n",
        "    'syllable_dict.txt' : 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/Syllable_dictionary.txt',\n",
        "    'about_syllable_dict.docx' : 'https://caltech-cs155.s3.us-east-2.amazonaws.com/miniprojects/project3/data/syllable_dict_explanation.docx'\n",
        "}\n",
        "\n",
        "def download_file(file_path):\n",
        "    url = url_dict[file_path]\n",
        "    print('Start downloading...')\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(file_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024 * 1024 * 1024):\n",
        "                f.write(chunk)\n",
        "    print('Complete')\n",
        "\n",
        "download_file('shakespeare.txt') #all the shakespeare sonnets\n",
        "download_file('spenser.txt') #all the spenser sonnets\n",
        "download_file('syllable_dict.txt') #all the words that appear in the shakespeare sonnets, followed by number of syllables\n",
        "download_file('about_syllable_dict.docx') #info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U5Lb0xqyjDk"
      },
      "source": [
        "### Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IpoTC_tjM4v"
      },
      "outputs": [],
      "source": [
        "#read the data from shakespeare.txt, and store in string\n",
        "shakespeare_f = open('shakespeare.txt', 'r')\n",
        "shakespeare_str = shakespeare_f.read()\n",
        "\n",
        "spenser_f = open('spenser.txt', 'r')\n",
        "spenser_str = spenser_f.read()\n",
        "\n",
        "syllable_dict_f = open('syllable_dict.txt', 'r')\n",
        "syllable_dict_str = syllable_dict_f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBScwHujO10"
      },
      "source": [
        "Parse the syllables dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrtnu22IyjDk"
      },
      "outputs": [],
      "source": [
        "syllable_dict = {} #hashmap of (word, syllables)\n",
        "for line in syllable_dict_str.split(\"\\n\"):\n",
        "    line_tpl = line.split(\" \")\n",
        "    if (line_tpl[len(line_tpl) - 1].startswith('E')):\n",
        "        #print(line_tpl[len(line_tpl) - 1])\n",
        "        line_tpl[len(line_tpl) - 1] = line_tpl[len(line_tpl) - 1][1]\n",
        "    if (line_tpl[len(line_tpl) - 1] == ''):\n",
        "        continue\n",
        "    syllable_dict[line_tpl[0]] = int(line_tpl[len(line_tpl) - 1])\n",
        "\n",
        "print(syllable_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q23N_ZAqZ-Uo"
      },
      "source": [
        "Project 6 code (no solutions yet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhSIpB8COdqq"
      },
      "source": [
        "###Preprocessing for HMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNNasu8ujmdv"
      },
      "source": [
        "Clean the shakespeare sonnets string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7UzBmFRToGh"
      },
      "outputs": [],
      "source": [
        "shakespeare_cleaned = \"\"\n",
        "def containsdigit(s):\n",
        "  return any(char.isdigit() for char in s)\n",
        "\n",
        "for line in shakespeare_str.split(\"\\n\"):\n",
        "    line = line.lower().strip()\n",
        "    if (line == \"\" or containsdigit(line)):\n",
        "        continue\n",
        "\n",
        "    for char in line:\n",
        "        if (char == ' ' or char == '-' or char == '\\''):\n",
        "            shakespeare_cleaned += char\n",
        "        elif (char in string.punctuation):\n",
        "            continue\n",
        "        else:\n",
        "            shakespeare_cleaned += char\n",
        "    shakespeare_cleaned += '\\n'\n",
        "\n",
        "shakespeare_processed = \"\"\n",
        "for line in shakespeare_cleaned.split('\\n'):\n",
        "    for word in line.split():\n",
        "        if (word in syllable_dict):\n",
        "            shakespeare_processed += word + \" \"\n",
        "        else:\n",
        "            continue\n",
        "    shakespeare_processed = shakespeare_processed[0:len(shakespeare_processed) - 1]\n",
        "    shakespeare_processed += '\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQcWP7fxjygD"
      },
      "source": [
        "Put the cleaned shakespeare text in an observation map and also observations array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dk4iJgXOdCg"
      },
      "outputs": [],
      "source": [
        "def parse_shakespeare_observations(text):\n",
        "    # Convert text to dataset.\n",
        "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
        "    obs_counter = 0\n",
        "    obs = []\n",
        "    obs_map = {}\n",
        "\n",
        "    for line in lines:\n",
        "        obs_elem = []\n",
        "\n",
        "        for word in line:\n",
        "            #word = re.sub(r'[^\\w]', '', word).lower()\n",
        "            if (word not in syllable_dict):\n",
        "                print(word)\n",
        "            if word not in obs_map:\n",
        "                # Add unique words to the observations map.\n",
        "                obs_map[word] = obs_counter\n",
        "                obs_counter += 1\n",
        "\n",
        "            # Add the encoded word.\n",
        "            obs_elem.append(obs_map[word])\n",
        "\n",
        "        # Add the encoded sequence.\n",
        "        obs.append(obs_elem)\n",
        "\n",
        "    return obs, obs_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUBZcXXvdPSt"
      },
      "outputs": [],
      "source": [
        "shakespeare_obs, shakespeare_obs_map = parse_shakespeare_observations(shakespeare_processed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# added for extra credit: incorporating spenser dict\n",
        "\n",
        "spenser_cleaned = \"\"\n",
        "def containsdigit(s):\n",
        "  return any(char.isdigit() for char in s)\n",
        "\n",
        "for line in spenser_str.split(\"\\n\"):\n",
        "    line = line.lower().strip()\n",
        "    if (line == \"\" or containsdigit(line)):\n",
        "        continue\n",
        "\n",
        "    for char in line:\n",
        "        if (char == ' ' or char == '-' or char == '\\''):\n",
        "            spenser_cleaned += char\n",
        "        elif (char in string.punctuation):\n",
        "            continue\n",
        "        else:\n",
        "            spenser_cleaned += char\n",
        "    spenser_cleaned += '\\n'\n",
        "\n",
        "spenser_processed = \"\"\n",
        "for line in spenser_cleaned.split('\\n'):\n",
        "    for word in line.split():\n",
        "        if (word in syllable_dict):\n",
        "            spenser_processed += word + \" \"\n",
        "        else:\n",
        "            continue\n",
        "    spenser_processed = spenser_processed[0:len(spenser_processed) - 1]\n",
        "    spenser_processed += '\\n'\n",
        "\n",
        "combined_processed = shakespeare_processed + spenser_processed\n",
        "combined_obs, combined_obs_map = parse_shakespeare_observations(combined_processed)"
      ],
      "metadata": {
        "id": "SJ0m1_6oJrmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO3zLrE3yjDk"
      },
      "source": [
        "# Implementing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUSRYU0EyjDl"
      },
      "source": [
        "## HMM Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u8kXCnlaBCN"
      },
      "outputs": [],
      "source": [
        "from pickle import LONG1\n",
        "import random\n",
        "# You can use this skeleton code to complete the HMM\n",
        "# implementation of set 5.\n",
        "#\n",
        "# Some pointers to get you started:\n",
        "#\n",
        "#     - Choose your notation carefully and consistently! Readable\n",
        "#       notation will make all the difference in the time it takes you\n",
        "#       to implement this class, as well as how difficult it is to debug.\n",
        "#\n",
        "#     - Read the documentation in this file! Make sure you know what\n",
        "#       is expected from each function and what each variable is.\n",
        "#\n",
        "#     - Any reference to \"the (i, j)^th\" element of a matrix T means that\n",
        "#       you should use T[i][j].\n",
        "#\n",
        "#     - Note that in our solution code, no NumPy was used. That is, there\n",
        "#       are no fancy tricks here, just basic coding. If you understand HMMs\n",
        "#       to a thorough extent, the rest of this implementation should come\n",
        "#       naturally. However, if you'd like to use NumPy, feel free to.\n",
        "#\n",
        "#     - Take one step at a time! Move onto the next algorithm to implement\n",
        "#       only if you're absolutely sure that all previous algorithms are\n",
        "#       correct. We are providing you waypoints for this reason.\n",
        "#\n",
        "# To get started, just fill in code where indicated. Best of luck!\n",
        "\n",
        "class HiddenMarkovModel:\n",
        "    '''\n",
        "    Class implementation of Hidden Markov Models.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, A, O):\n",
        "        '''\n",
        "        Initializes an HMM. Assumes the following:\n",
        "            - States and observations are integers starting from 0.\n",
        "            - There is a start state (see notes on A_start below). There\n",
        "              is no integer associated with the start state, only\n",
        "              probabilities in the vector A_start.\n",
        "            - There is no end state.\n",
        "        Arguments:\n",
        "            A:          Transition matrix with dimensions L x L.\n",
        "                        The (i, j)^th element is the probability of\n",
        "                        transitioning from state i to state j. Note that\n",
        "                        this does not include the starting probabilities.\n",
        "            O:          Observation matrix with dimensions L x D.\n",
        "                        The (i, j)^th element is the probability of\n",
        "                        emitting observation j given state i.\n",
        "        Parameters:\n",
        "            L:          Number of states.\n",
        "\n",
        "            D:          Number of observations.\n",
        "\n",
        "            A:          The transition matrix.\n",
        "\n",
        "            O:          The observation matrix.\n",
        "\n",
        "            A_start:    Starting transition probabilities. The i^th element\n",
        "                        is the probability of transitioning from the start\n",
        "                        state to state i. For simplicity, we assume that\n",
        "                        this distribution is uniform.\n",
        "        '''\n",
        "\n",
        "        self.L = len(A)\n",
        "        self.D = len(O[0])\n",
        "        self.A = A\n",
        "        self.O = O\n",
        "        self.A_start = [1. / self.L for _ in range(self.L)]\n",
        "\n",
        "\n",
        "    def viterbi(self, x):\n",
        "        '''\n",
        "        Uses the Viterbi algorithm to find the max probability state\n",
        "        sequence corresponding to a given input sequence.\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "        Returns:\n",
        "            max_seq:    State sequence corresponding to x with the highest\n",
        "                        probability.\n",
        "        '''\n",
        "\n",
        "        M = len(x)      # Length of sequence.\n",
        "\n",
        "        # The (i, j)^th elements of probs and seqs are the max probability\n",
        "        # of the prefix of length i ending in state j and the prefix\n",
        "        # that gives this probability, respectively.\n",
        "        #\n",
        "        # For instance, probs[1][0] is the probability of the prefix of\n",
        "        # length 1 ending in state 0.\n",
        "        probs = [[0. for _ in range(self.L)] for _ in range(M + 1)]\n",
        "        seqs = [['' for _ in range(self.L)] for _ in range(M + 1)]\n",
        "\n",
        "        # include initial probabilities\n",
        "        for l in range(self.L):\n",
        "          probs[1][l] = self.A_start[l] * self.O[l][x[0]]\n",
        "          seqs[1][l] = str(l)\n",
        "\n",
        "        for k in range(2, M + 1) : # traverse the input sequence\n",
        "            for l in range(self.L): # traverse all states\n",
        "              max_p = float('-inf')\n",
        "              max_p_seq = ''\n",
        "              for j in range(self.L): # find most likely path\n",
        "                  curr_p = probs[k - 1][j] * self.A[j][l] * self.O[l][x[k - 1]]\n",
        "                  if curr_p > max_p:\n",
        "                      max_p = curr_p\n",
        "                      max_p_seq = seqs[k-1][j] + str(l)\n",
        "              probs[k][l] = max_p\n",
        "              seqs[k][l] = max_p_seq\n",
        "\n",
        "        end_state = probs[M].index(max(probs[M]))\n",
        "        max_seq = seqs[M][end_state]\n",
        "\n",
        "        return max_seq\n",
        "\n",
        "\n",
        "    def forward(self, x, normalize=False):\n",
        "        '''\n",
        "        Uses the forward algorithm to calculate the alpha probability\n",
        "        vectors corresponding to a given input sequence.\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "            normalize:  Whether to normalize each set of alpha_j(i) vectors\n",
        "                        at each i. This is useful to avoid underflow in\n",
        "                        unsupervised learning.\n",
        "        Returns:\n",
        "            alphas:     Vector of alphas.\n",
        "                        The (i, j)^th element of alphas is alpha_j(i),\n",
        "                        i.e. the probability of observing prefix x^1:i\n",
        "                        and state y^i = j.\n",
        "                        e.g. alphas[1][0] corresponds to the probability\n",
        "                        of observing x^1:1, i.e. the first observation,\n",
        "                        given that y^1 = 0, i.e. the first state is 0.\n",
        "        '''\n",
        "        M = len(x)      # Length of sequence.\n",
        "        alphas = [[0. for _ in range(self.L)] for _ in range(M + 1)]\n",
        "\n",
        "        for l in range(self.L):\n",
        "          alphas[1][l] = self.A_start[l] * self.O[l][x[0]]\n",
        "        if normalize:\n",
        "            normalize_by = sum(alphas[1])\n",
        "            alphas[1] = [alpha / normalize_by for alpha in alphas[1]]\n",
        "\n",
        "        for k in range(2, M + 1):\n",
        "            for l in range(self.L):\n",
        "                alphas_sum = 0\n",
        "                for j in range(self.L):\n",
        "                    alphas_sum += alphas[k - 1][j] * self.A[j][l] * self.O[l][x[k - 1]]\n",
        "                alphas[k][l] = alphas_sum\n",
        "            if normalize:\n",
        "                normalize_by = sum(alphas[k])\n",
        "                alphas[k] = [alpha / normalize_by for alpha in alphas[k]]\n",
        "\n",
        "        return alphas\n",
        "\n",
        "\n",
        "    def backward(self, x, normalize=False):\n",
        "        '''\n",
        "        Uses the backward algorithm to calculate the beta probability\n",
        "        vectors corresponding to a given input sequence.\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "            normalize:  Whether to normalize each set of alpha_j(i) vectors\n",
        "                        at each i. This is useful to avoid underflow in\n",
        "                        unsupervised learning.\n",
        "        Returns:\n",
        "            betas:      Vector of betas.\n",
        "                        The (i, j)^th element of betas is beta_j(i), i.e.\n",
        "                        the probability of observing prefix x^(i+1):M and\n",
        "                        state y^i = j.\n",
        "                        e.g. betas[M][0] corresponds to the probability\n",
        "                        of observing x^M+1:M, i.e. no observations,\n",
        "                        given that y^M = 0, i.e. the last state is 0.\n",
        "        '''\n",
        "\n",
        "        M = len(x)      # Length of sequence.\n",
        "        betas = [[0. for _ in range(self.L)] for _ in range(M + 1)]\n",
        "\n",
        "        for l in range(self.L):\n",
        "          betas[M][l] = 1\n",
        "        if normalize:\n",
        "          normalize_by = sum(betas[M])\n",
        "          for i in range(len(betas[M])):\n",
        "            betas[M][i] = betas[M][i] / normalize_by\n",
        "\n",
        "        for k in range(M - 1, 0, -1):\n",
        "          for l in range(self.L):\n",
        "            beta_sum = 0\n",
        "            for j in range(self.L):\n",
        "              beta_sum += betas[k+1][j] * self.A[l][j] * self.O[j][x[k]]\n",
        "            betas[k][l] = beta_sum\n",
        "          if normalize:\n",
        "            normalize_by = sum(betas[k])\n",
        "            for i in range(len(betas[k])):\n",
        "              betas[k][i] = betas[k][i] / normalize_by\n",
        "\n",
        "        return betas\n",
        "\n",
        "\n",
        "    def supervised_learning(self, X, Y):\n",
        "        '''\n",
        "        Trains the HMM using the Maximum Likelihood closed form solutions\n",
        "        for the transition and observation matrices on a labeled\n",
        "        datset (X, Y). Note that this method does not return anything, but\n",
        "        instead updates the attributes of the HMM object.\n",
        "        Arguments:\n",
        "            X:          A dataset consisting of input sequences in the form\n",
        "                        of lists of variable length, consisting of integers\n",
        "                        ranging from 0 to D - 1. In other words, a list of\n",
        "                        lists.\n",
        "            Y:          A dataset consisting of state sequences in the form\n",
        "                        of lists of variable length, consisting of integers\n",
        "                        ranging from 0 to L - 1. In other words, a list of\n",
        "                        lists.\n",
        "                        Note that the elements in X line up with those in Y.\n",
        "        '''\n",
        "\n",
        "        # Calculate each element of A using the M-step formulas.\n",
        "        for l1 in range(self.L):\n",
        "          for l2 in range(self.L):\n",
        "            num = denom = 0\n",
        "            for i, y in enumerate(Y):\n",
        "              transitions = [y[j-1] == l1 and y[j] == l2 for j in range(1, len(y))]\n",
        "              num += sum(transitions)\n",
        "              denom += sum(y[j-1] == l1 for j in range(1, len(y)))\n",
        "            if denom > 0:\n",
        "              self.A[l1][l2] = num / denom\n",
        "            else:\n",
        "              self.A[l1][l2] = 0\n",
        "\n",
        "        # Calculate each element of O using the M-step formulas.\n",
        "        for l in range(self.L):\n",
        "          for d in range(self.D):\n",
        "            num = denom = 0\n",
        "            for i in range(len(X)):\n",
        "              for j in range(len(Y[i])):\n",
        "                if Y[i][j] == l:\n",
        "                  denom += 1\n",
        "                  if X[i][j] == d:\n",
        "                    num += 1\n",
        "              if denom > 0:\n",
        "                self.O[l][d] = num / denom\n",
        "              else:\n",
        "                self.O[l][d] = 0\n",
        "\n",
        "\n",
        "    def unsupervised_learning(self, X, N_iters):\n",
        "        '''\n",
        "        Trains the HMM using the Baum-Welch algorithm on an unlabeled\n",
        "        datset X. Note that this method does not return anything, but\n",
        "        instead updates the attributes of the HMM object.\n",
        "        Arguments:\n",
        "            X:          A dataset consisting of input sequences in the form\n",
        "                        of variable-length lists, consisting of integers\n",
        "                        ranging from 0 to D - 1. In other words, a list of\n",
        "                        lists.\n",
        "            N_iters:    The number of iterations to train on.\n",
        "        '''\n",
        "        for iter in range(N_iters):\n",
        "          A_numer = np.zeros((self.L, self.L))\n",
        "          A_denom = np.zeros((self.L, self.L))\n",
        "          O_numer = np.zeros((self.L, self.D))\n",
        "          O_denom = np.zeros((self.L, self.D))\n",
        "\n",
        "          for x in X:\n",
        "            M = len(x)\n",
        "            alphas = self.forward(x, normalize=True)\n",
        "            betas = self.backward(x, normalize=True)\n",
        "            for i in range(1, len(x) + 1):\n",
        "              Paz = []\n",
        "              for l in range(self.L):\n",
        "                Paz.append(alphas[i][l] * betas[i][l])\n",
        "              normalize_by = np.sum(Paz)\n",
        "              if normalize_by != 0:\n",
        "                Paz /= normalize_by\n",
        "\n",
        "\n",
        "              for l in range(self.L):\n",
        "                O_numer[l][x[i - 1]] += Paz[l]\n",
        "                O_denom[l] += Paz[l]\n",
        "\n",
        "              if i < len(x):\n",
        "                Pab = np.zeros((self.L, self.L))\n",
        "                for l1 in range(self.L):\n",
        "                  for l2 in range(self.L):\n",
        "                    Pab[l1][l2] = alphas[i][l1] * self.A[l1][l2] * self.O[l2][x[i]] * betas[i + 1][l2]\n",
        "                normalize_by = np.sum(Pab)\n",
        "                if normalize_by != 0:\n",
        "                  Pab /= normalize_by\n",
        "\n",
        "\n",
        "                for l1 in range(self.L):\n",
        "                  A_denom[l1] += Paz[l1]\n",
        "                  for l2 in range(self.L):\n",
        "                    A_numer[l1][l2] += Pab[l1][l2]\n",
        "\n",
        "          self.A = np.divide(A_numer, A_denom, where=A_denom != 0)\n",
        "          self.O = np.divide(O_numer, O_denom, where=O_denom != 0)\n",
        "\n",
        "\n",
        "    def generate_emission(self, M, seed=None):\n",
        "        '''\n",
        "        Generates an emission of length M, assuming that the starting state\n",
        "        is chosen uniformly at random.\n",
        "        Arguments:\n",
        "            M:          Length of the emission to generate.\n",
        "        Returns:\n",
        "            emission:   The randomly generated emission as a list.\n",
        "            states:     The randomly generated states as a list.\n",
        "        '''\n",
        "\n",
        "        # (Re-)Initialize random number generator\n",
        "        rng = np.random.default_rng(seed=seed)\n",
        "\n",
        "        emission = []\n",
        "        states = []\n",
        "\n",
        "        curr_state = rng.choice(range(self.L))\n",
        "        for m in range(M):\n",
        "            states.append(curr_state)\n",
        "            emission.append(rng.choice(range(self.D), p = self.O[curr_state]))\n",
        "            curr_state = rng.choice(range(self.L), p = self.A[curr_state])\n",
        "\n",
        "        return emission, states\n",
        "\n",
        "\n",
        "\n",
        "    def probability_alphas(self, x):\n",
        "        '''\n",
        "        Finds the maximum probability of a given input sequence using\n",
        "        the forward algorithm.\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "        Returns:\n",
        "            prob:       Total probability that x can occur.\n",
        "        '''\n",
        "\n",
        "        # Calculate alpha vectors.\n",
        "        alphas = self.forward(x)\n",
        "\n",
        "        # alpha_j(M) gives the probability that the state sequence ends\n",
        "        # in j. Summing this value over all possible states j gives the\n",
        "        # total probability of x paired with any state sequence, i.e.\n",
        "        # the probability of x.\n",
        "        prob = sum(alphas[-1])\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def probability_betas(self, x):\n",
        "        '''\n",
        "        Finds the maximum probability of a given input sequence using\n",
        "        the backward algorithm.\n",
        "        Arguments:\n",
        "            x:          Input sequence in the form of a list of length M,\n",
        "                        consisting of integers ranging from 0 to D - 1.\n",
        "        Returns:\n",
        "            prob:       Total probability that x can occur.\n",
        "        '''\n",
        "\n",
        "        betas = self.backward(x)\n",
        "\n",
        "        # beta_j(1) gives the probability that the state sequence starts\n",
        "        # with j. Summing this, multiplied by the starting transition\n",
        "        # probability and the observation probability, over all states\n",
        "        # gives the total probability of x paired with any state\n",
        "        # sequence, i.e. the probability of x.\n",
        "        prob = sum([betas[1][j] * self.A_start[j] * self.O[j][x[0]] \\\n",
        "                    for j in range(self.L)])\n",
        "\n",
        "        return prob\n",
        "\n",
        "\n",
        "def supervised_HMM(X, Y):\n",
        "    '''\n",
        "    Helper function to train a supervised HMM. The function determines the\n",
        "    number of unique states and observations in the given data, initializes\n",
        "    the transition and observation matrices, creates the HMM, and then runs\n",
        "    the training function for supervised learning.\n",
        "    Arguments:\n",
        "        X:          A dataset consisting of input sequences in the form\n",
        "                    of lists of variable length, consisting of integers\n",
        "                    ranging from 0 to D - 1. In other words, a list of lists.\n",
        "        Y:          A dataset consisting of state sequences in the form\n",
        "                    of lists of variable length, consisting of integers\n",
        "                    ranging from 0 to L - 1. In other words, a list of lists.\n",
        "                    Note that the elements in X line up with those in Y.\n",
        "    '''\n",
        "    # Make a set of observations.\n",
        "    observations = set()\n",
        "    for x in X:\n",
        "        observations |= set(x)\n",
        "\n",
        "    # Make a set of states.\n",
        "    states = set()\n",
        "    for y in Y:\n",
        "        states |= set(y)\n",
        "\n",
        "    # Compute L and D.\n",
        "    L = len(states)\n",
        "    D = len(observations)\n",
        "\n",
        "    # Randomly initialize and normalize matrix A.\n",
        "    A = [[random.random() for i in range(L)] for j in range(L)]\n",
        "\n",
        "    for i in range(len(A)):\n",
        "        norm = sum(A[i])\n",
        "        for j in range(len(A[i])):\n",
        "            A[i][j] /= norm\n",
        "\n",
        "    # Randomly initialize and normalize matrix O.\n",
        "    O = [[random.random() for i in range(D)] for j in range(L)]\n",
        "\n",
        "    for i in range(len(O)):\n",
        "        norm = sum(O[i])\n",
        "        for j in range(len(O[i])):\n",
        "            O[i][j] /= norm\n",
        "\n",
        "    # Train an HMM with labeled data.\n",
        "    HMM = HiddenMarkovModel(A, O)\n",
        "    HMM.supervised_learning(X, Y)\n",
        "\n",
        "    return HMM\n",
        "\n",
        "def unsupervised_HMM(X, n_states, N_iters, seed=None):\n",
        "    '''\n",
        "    Helper function to train an unsupervised HMM. The function determines the\n",
        "    number of unique observations in the given data, initializes\n",
        "    the transition and observation matrices, creates the HMM, and then runs\n",
        "    the training function for unsupervised learing.\n",
        "    Arguments:\n",
        "        X:          A dataset consisting of input sequences in the form\n",
        "                    of lists of variable length, consisting of integers\n",
        "                    ranging from 0 to D - 1. In other words, a list of lists.\n",
        "        n_states:   Number of hidden states to use in training.\n",
        "\n",
        "        N_iters:    The number of iterations to train on.\n",
        "        rng:        The random number generator for reproducible result.\n",
        "                    Default to RandomState(1).\n",
        "    '''\n",
        "    # Initialize random number generator\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "\n",
        "    # Make a set of observations.\n",
        "    observations = set()\n",
        "    for x in X:\n",
        "        observations |= set(x)\n",
        "\n",
        "    # Compute L and D.\n",
        "    L = n_states\n",
        "    D = len(observations)\n",
        "\n",
        "    # Randomly initialize and normalize matrix A.\n",
        "    A = [[rng.random() for i in range(L)] for j in range(L)]\n",
        "\n",
        "    for i in range(len(A)):\n",
        "        norm = sum(A[i])\n",
        "        for j in range(len(A[i])):\n",
        "            A[i][j] /= norm\n",
        "\n",
        "    # Randomly initialize and normalize matrix O.\n",
        "    O = [[rng.random() for i in range(D)] for j in range(L)]\n",
        "\n",
        "    for i in range(len(O)):\n",
        "        norm = sum(O[i])\n",
        "        for j in range(len(O[i])):\n",
        "            O[i][j] /= norm\n",
        "\n",
        "    # Train an HMM with unlabeled data.\n",
        "    HMM = HiddenMarkovModel(A, O)\n",
        "    HMM.unsupervised_learning(X, N_iters)\n",
        "\n",
        "    return HMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQafYIbwnMnM"
      },
      "outputs": [],
      "source": [
        "def obs_map_reverser(obs_map):\n",
        "    obs_map_r = {}\n",
        "\n",
        "    for key in obs_map:\n",
        "        obs_map_r[obs_map[key]] = key\n",
        "\n",
        "    return obs_map_r\n",
        "\n",
        "def sample_sentence(hmm, obs_map, n_words=100, seed=None):\n",
        "    # Get reverse map.\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "\n",
        "    # Sample and convert sentence.\n",
        "    emission, states = hmm.generate_emission(n_words, seed=seed)\n",
        "    sentence = [obs_map_r[i] for i in emission]\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "def count_syllables(sentence):\n",
        "    total = 0\n",
        "    for word in sentence.split(\" \"):\n",
        "        total += syllable_dict[word]\n",
        "    return total\n",
        "\n",
        "def generate_sonnet(hmm):\n",
        "    #must have 14 lines in total and 10 syllables per line\n",
        "    result = \"\"\n",
        "    for i in range(14):\n",
        "        n_wds = 7\n",
        "        sentence = sample_sentence(hmm, shakespeare_obs_map, n_words=n_wds)\n",
        "        while (count_syllables(sentence) != 10):\n",
        "            if (count_syllables(sentence) < 10):\n",
        "                n_wds += 1\n",
        "            if (count_syllables(sentence) > 10):\n",
        "                n_wds -= 1\n",
        "            sentence = sample_sentence(hmm, shakespeare_obs_map, n_words=n_wds)\n",
        "\n",
        "        result += sentence + '\\n'\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hmm4 = unsupervised_HMM(shakespeare_obs, 4, 100, seed=123)\n",
        "hmm8 = unsupervised_HMM(shakespeare_obs, 8, 100, seed=123)\n",
        "hmm10 = unsupervised_HMM(shakespeare_obs, 10, 100, seed=123)\n",
        "hmm12 = unsupervised_HMM(shakespeare_obs, 12, 100, seed=123)\n",
        "hmm16 = unsupervised_HMM(shakespeare_obs, 16, 100, seed=123)\n",
        "\n",
        "hmms = [hmm4, hmm8, hmm10, hmm12, hmm16]\n",
        "hmm_nums = [4,8,10,12,16]\n",
        "\n",
        "for hmm, num in zip(hmms, hmm_nums):\n",
        "    print(f\"HIDDEN STATE: {num}\" )\n",
        "    print(generate_sonnet(hmm))"
      ],
      "metadata": {
        "id": "AiSPOmNxO4-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Goal: rhyme scheme\n",
        "\n",
        "def generate_reverse_emission(self, M, endword, seed=None):\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    emission = []\n",
        "    states = []\n",
        "\n",
        "    curr_word = endword\n",
        "    emission.append(curr_word)\n",
        "    states.append(rng.choice(range(self.L)))\n",
        "    curr_state = rng.choice(range(self.L))\n",
        "\n",
        "    for m in range(M-1):\n",
        "      states.append(curr_state)\n",
        "      emission.append(rng.choice(range(self.D), p = self.O[curr_state]))\n",
        "      poss_prev_states = self.A[:,curr_state]\n",
        "      most_likely_prev_state = np.argmax(poss_prev_states)\n",
        "      curr_state = most_likely_prev_state\n",
        "\n",
        "    return emission[::-1], states[::-1]\n",
        "\n",
        "def sample_reverse_sentence(hmm, obs_map, endword, n_words=100, seed=None):\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "    emission, states = hmm.generate_reverse_emission(n_words, endword, seed=seed)\n",
        "\n",
        "    sentence1 = [obs_map_r[i] for i in emission]\n",
        "\n",
        "    return ' '.join(sentence1)\n",
        "\n",
        "def generate_rhyming_sonnet(hmm):\n",
        "  #must have 14 lines in total and 10 syllables per line\n",
        "    result = \"\"\n",
        "    len_map = len(rhyme_map)\n",
        "    rhyme_keys = list(rhyme_map.keys())\n",
        "    # 14 zeros\n",
        "    endwords = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "    # fill out the array of endwords for the sonnet\n",
        "    for i in range(7):\n",
        "      idx = random.randint(0, len_map-1)\n",
        "      # print(idx)\n",
        "      endword = rhyme_keys[idx]\n",
        "      endword_pair = rhyme_map[endword]\n",
        "      if i < 6:\n",
        "        if i % 2 == 0:\n",
        "          endwords[i*2] = endword\n",
        "          endwords[i*2 + 2] = endword_pair\n",
        "        else:\n",
        "          endwords[i*2-1] = endword\n",
        "          endwords[i*2+1] = endword_pair\n",
        "\n",
        "      else:\n",
        "        endwords[i*2] = endword\n",
        "        endwords[i*2+1] = endword_pair\n",
        "\n",
        "    # print_words = []\n",
        "    # for word in endwords:\n",
        "    #   print_words.append(flipped_shakespeare_obs_map[word])\n",
        "    # print(print_words)\n",
        "\n",
        "    # Make sentences with the endwords\n",
        "    for i in range(14):\n",
        "      endword = endwords[i]\n",
        "      n_wds = 7\n",
        "      sentence = sample_reverse_sentence(hmm, shakespeare_obs_map, endword, n_words=n_wds)\n",
        "      while (count_syllables(sentence) != 10):\n",
        "          if (count_syllables(sentence) < 10):\n",
        "              n_wds += 1\n",
        "          if (count_syllables(sentence) > 10):\n",
        "              n_wds -= 1\n",
        "          sentence = sample_reverse_sentence(hmm, shakespeare_obs_map, endword, n_words=n_wds)\n",
        "\n",
        "      result += sentence + '\\n'\n",
        "    return result\n",
        "\n",
        "hmm4 = unsupervised_HMM(shakespeare_obs, 4, 100, seed=123)\n",
        "# hmm8 = unsupervised_HMM(shakespeare_obs, 8, 100, seed=123)\n",
        "hmm10 = unsupervised_HMM(shakespeare_obs, 10, 100, seed=123)\n",
        "# hmm12 = unsupervised_HMM(shakespeare_obs, 12, 100, seed=123)\n",
        "hmm16 = unsupervised_HMM(shakespeare_obs, 16, 100, seed=123)\n",
        "\n",
        "# hmms = [hmm4, hmm8, hmm10, hmm12, hmm16]\n",
        "# hmm_nums = [4,8,10,12,16]\n",
        "hmms = [hmm4, hmm10, hmm16]\n",
        "hmm_nums = [4,10,16]\n",
        "\n",
        "for hmm, num in zip(hmms, hmm_nums):\n",
        "    print(f\"HIDDEN STATE: {num}\" )\n",
        "    print(\"With rhyme scheme: \")\n",
        "    print(generate_sonnet(hmm))\n"
      ],
      "metadata": {
        "id": "Enyy7YI7A_n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extra credit: incorporating spenser\n",
        "\n",
        "def spenser_generate_sonnet(hmm):\n",
        "    #must have 14 lines in total and 10 syllables per line\n",
        "    result = \"\"\n",
        "    for i in range(14):\n",
        "        n_wds = 7\n",
        "        sentence = sample_sentence(hmm, combined_obs_map, n_words=n_wds)\n",
        "        while (count_syllables(sentence) != 10):\n",
        "            if (count_syllables(sentence) < 10):\n",
        "                n_wds += 1\n",
        "            if (count_syllables(sentence) > 10):\n",
        "                n_wds -= 1\n",
        "            sentence = sample_sentence(hmm, combined_obs_map, n_words=n_wds)\n",
        "\n",
        "        result += sentence + '\\n'\n",
        "    return result\n",
        "\n",
        "spenser_hmm4 = unsupervised_HMM(combined_obs, 4, 100, seed=123)\n",
        "spenser_hmm8 = unsupervised_HMM(combined_obs, 8, 100, seed=123)\n",
        "spenser_hmm10 = unsupervised_HMM(combined_obs, 10, 100, seed=123)\n",
        "\n",
        "spenser_hmms = [spenser_hmm4, spenser_hmm8, spenser_hmm10]\n",
        "spenser_hmm_nums = [4,8,10]\n",
        "\n",
        "for spenser_hmm, spenser_num in zip(spenser_hmms, spenser_hmm_nums):\n",
        "    print(f\"HIDDEN STATE: {spenser_num}\" )\n",
        "    print(spenser_generate_sonnet(spenser_hmm))"
      ],
      "metadata": {
        "id": "RHZjGACAKffT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Additional Goals: Other poetic forms"
      ],
      "metadata": {
        "id": "zILBESO8v7gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_line(hmm, syllables):\n",
        "    n_wds = int(syllables * 2/3) + 1 #heuristic\n",
        "    sentence = sample_sentence(hmm, shakespeare_obs_map, n_words=n_wds)\n",
        "    while (count_syllables(sentence) != syllables):\n",
        "        if (count_syllables(sentence) < syllables):\n",
        "            n_wds += 1\n",
        "        if (count_syllables(sentence) > syllables):\n",
        "            n_wds -= 1\n",
        "        sentence = sample_sentence(hmm, shakespeare_obs_map, n_words=n_wds)\n",
        "    return sentence\n",
        "\n",
        "def generate_poem(hmm, syllables_pattern):\n",
        "    #given a pattern of syllables cooresponding to lines in a poem, generate that poem\n",
        "    result = \"\"\n",
        "    for line_syll in syllables_pattern:\n",
        "        result += generate_line(hmm, line_syll) + \"\\n\"\n",
        "    return result[:-1]\n",
        "\n",
        "def generate_haiku(hmm):\n",
        "    #must have 3 lines, 5,7,5 syllables\n",
        "    return generate_poem(hmm, [5,7,5])\n",
        "\n",
        "def generate_nonet(hmm):\n",
        "    return generate_poem(hmm, [9,8,7,6,5,4,3,2,1])"
      ],
      "metadata": {
        "id": "jTuRtyPvwF2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hmm = unsupervised_HMM(shakespeare_obs, 100, 16, seed=123)"
      ],
      "metadata": {
        "id": "w8udCVbswYY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_haiku(hmm))"
      ],
      "metadata": {
        "id": "2_CiV2Mewak0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_nonet(hmm))"
      ],
      "metadata": {
        "id": "CAiogSwHwb40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_poem(hmm, [4,8,4,2,2]))"
      ],
      "metadata": {
        "id": "FcqiuNOJwdE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctM0uGcwmmiY"
      },
      "source": [
        "##HMM Visualization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "mmWKz6EXbrgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "from matplotlib import animation\n",
        "from matplotlib.animation import FuncAnimation"
      ],
      "metadata": {
        "id": "N5GHOSlibsfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "# CS/CNS/EE 155 2018\n",
        "# Problem Set 6\n",
        "#\n",
        "# Author:       Andrew Kang\n",
        "# Description:  Set 6 HMM helper\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# WORDCLOUD FUNCTIONS\n",
        "####################\n",
        "\n",
        "def mask():\n",
        "    # Parameters.\n",
        "    r = 128\n",
        "    d = 2 * r + 1\n",
        "\n",
        "    # Get points in a circle.\n",
        "    y, x = np.ogrid[-r:d-r, -r:d-r]\n",
        "    circle = (x**2 + y**2 <= r**2)\n",
        "\n",
        "    # Create mask.\n",
        "    mask = 255 * np.ones((d, d), dtype=np.uint8)\n",
        "    mask[circle] = 0\n",
        "\n",
        "    return mask\n",
        "\n",
        "def text_to_wordcloud(text, max_words=50, title='', show=True):\n",
        "    plt.close('all')\n",
        "\n",
        "    # Generate a wordcloud image.\n",
        "    wordcloud = WordCloud(random_state=0,\n",
        "                          max_words=max_words,\n",
        "                          background_color='white',\n",
        "                          mask=mask()).generate(text)\n",
        "\n",
        "    # Show the image.\n",
        "    if show:\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(title, fontsize=24)\n",
        "        plt.show()\n",
        "\n",
        "    return wordcloud\n",
        "\n",
        "def states_to_wordclouds(hmm, obs_map, max_words=50, show=True):\n",
        "    # Initialize.\n",
        "    M = 100000\n",
        "    n_states = len(hmm.A)\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "    wordclouds = []\n",
        "\n",
        "    # Generate a large emission.\n",
        "    emission, states = hmm.generate_emission(M)\n",
        "\n",
        "    # For each state, get a list of observations that have been emitted\n",
        "    # from that state.\n",
        "    obs_count = []\n",
        "    for i in range(n_states):\n",
        "        obs_lst = np.array(emission)[np.where(np.array(states) == i)[0]]\n",
        "        obs_count.append(obs_lst)\n",
        "\n",
        "    # For each state, convert it into a wordcloud.\n",
        "    for i in range(n_states):\n",
        "        obs_lst = obs_count[i]\n",
        "        sentence = [obs_map_r[j] for j in obs_lst]\n",
        "        sentence_str = ' '.join(sentence)\n",
        "\n",
        "        wordclouds.append(text_to_wordcloud(sentence_str, max_words=max_words, title='State %d' % i, show=show))\n",
        "\n",
        "    return wordclouds\n",
        "\n",
        "\n",
        "def states_to_wordclouds_modified(hmm, obs_map, max_words=50, show=True):\n",
        "    # Initialize.\n",
        "    M = 100000\n",
        "    n_states = len(hmm.A)\n",
        "    our_states = range(n_states)\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "    wordclouds = []\n",
        "\n",
        "    # Generate a large emission.\n",
        "    emission, states = hmm.generate_emission(M)\n",
        "\n",
        "    # For each state, get a list of observations that have been emitted\n",
        "    # from that state.\n",
        "    obs_count = []\n",
        "    for i in range(n_states):\n",
        "        obs_lst = np.array(emission)[np.where(np.array(states) == i)[0]]\n",
        "        obs_count.append(obs_lst)\n",
        "\n",
        "    # For each state, convert it into a wordcloud.\n",
        "    for i in range(n_states):\n",
        "        obs_lst = obs_count[i]\n",
        "        sentence = [obs_map_r[j] for j in obs_lst]\n",
        "        sentence_str = ' '.join(sentence)\n",
        "        if (i in our_states):\n",
        "            wordclouds.append(text_to_wordcloud(sentence_str, max_words=max_words, title='State %d' % i, show=show))\n",
        "\n",
        "    return wordclouds\n",
        "\n",
        "####################\n",
        "# HMM FUNCTIONS\n",
        "####################\n",
        "\n",
        "def parse_observations(text):\n",
        "    # Convert text to dataset.\n",
        "    lines = [line.split() for line in text.split('\\n') if line.split()]\n",
        "\n",
        "    obs_counter = 0\n",
        "    obs = []\n",
        "    obs_map = {}\n",
        "\n",
        "    for line in lines:\n",
        "        obs_elem = []\n",
        "\n",
        "        for word in line:\n",
        "            word = re.sub(r'[^\\w]', '', word).lower()\n",
        "            if word not in obs_map:\n",
        "                # Add unique words to the observations map.\n",
        "                obs_map[word] = obs_counter\n",
        "                obs_counter += 1\n",
        "\n",
        "            # Add the encoded word.\n",
        "            obs_elem.append(obs_map[word])\n",
        "\n",
        "        # Add the encoded sequence.\n",
        "        obs.append(obs_elem)\n",
        "\n",
        "    return obs, obs_map\n",
        "\n",
        "def obs_map_reverser(obs_map):\n",
        "    obs_map_r = {}\n",
        "\n",
        "    for key in obs_map:\n",
        "        obs_map_r[obs_map[key]] = key\n",
        "\n",
        "    return obs_map_r\n",
        "\n",
        "def sample_sentence(hmm, obs_map, n_words=100, seed=None):\n",
        "    # Get reverse map.\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "\n",
        "    # Sample and convert sentence.\n",
        "    emission, states = hmm.generate_emission(n_words, seed=seed)\n",
        "    sentence = [obs_map_r[i] for i in emission]\n",
        "\n",
        "    return ' '.join(sentence).capitalize() + '...'\n",
        "\n",
        "\n",
        "####################\n",
        "# HMM VISUALIZATION FUNCTIONS\n",
        "####################\n",
        "\n",
        "def visualize_sparsities(hmm, O_max_cols=50, O_vmax=0.1):\n",
        "    plt.close('all')\n",
        "    plt.set_cmap('viridis')\n",
        "\n",
        "    # Visualize sparsity of A.\n",
        "    plt.imshow(hmm.A, vmax=1.0)\n",
        "    plt.colorbar()\n",
        "    plt.title('Sparsity of A matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualize parsity of O.\n",
        "    plt.imshow(np.array(hmm.O)[:, :O_max_cols], vmax=O_vmax, aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title('Sparsity of O matrix')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "####################\n",
        "# HMM ANIMATION FUNCTIONS\n",
        "####################\n",
        "\n",
        "def animate_emission(hmm, obs_map, M=8, height=12, width=12, delay=1, seed=None):\n",
        "    # Parameters.\n",
        "    lim = 1200\n",
        "    text_x_offset = 40\n",
        "    text_y_offset = 80\n",
        "    x_offset = 580\n",
        "    y_offset = 520\n",
        "    R = 420\n",
        "    r = 100\n",
        "    arrow_size = 20\n",
        "    arrow_p1 = 0.03\n",
        "    arrow_p2 = 0.02\n",
        "    arrow_p3 = 0.06\n",
        "\n",
        "    # Initialize.\n",
        "    n_states = len(hmm.A)\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "    wordclouds = states_to_wordclouds(hmm, obs_map, max_words=20, show=False)\n",
        "\n",
        "    # Initialize plot.\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_figheight(height)\n",
        "    fig.set_figwidth(width)\n",
        "    ax.grid('off')\n",
        "    plt.axis('off')\n",
        "    ax.set_xlim([0, lim])\n",
        "    ax.set_ylim([0, lim])\n",
        "\n",
        "    # Plot each wordcloud.\n",
        "    for i, wordcloud in enumerate(wordclouds):\n",
        "        x = x_offset + int(R * np.cos(np.pi * 2 * i / n_states))\n",
        "        y = y_offset + int(R * np.sin(np.pi * 2 * i / n_states))\n",
        "        ax.imshow(wordcloud.to_array(), extent=(x - r, x + r, y - r, y + r), aspect='auto', zorder=-1)\n",
        "\n",
        "    # Initialize text.\n",
        "    text = ax.text(text_x_offset, lim - text_y_offset, '', fontsize=24)\n",
        "\n",
        "    # Make the arrows.\n",
        "    zorder_mult = n_states ** 2 * 100\n",
        "    arrows = []\n",
        "    for i in range(n_states):\n",
        "        row = []\n",
        "        for j in range(n_states):\n",
        "            # Arrow coordinates.\n",
        "            x_i = x_offset + R * np.cos(np.pi * 2 * i / n_states)\n",
        "            y_i = y_offset + R * np.sin(np.pi * 2 * i / n_states)\n",
        "            x_j = x_offset + R * np.cos(np.pi * 2 * j / n_states)\n",
        "            y_j = y_offset + R * np.sin(np.pi * 2 * j / n_states)\n",
        "\n",
        "            dx = x_j - x_i\n",
        "            dy = y_j - y_i\n",
        "            d = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "            if i != j:\n",
        "                arrow = ax.arrow(x_i + (r/d + arrow_p1) * dx + arrow_p2 * dy,\n",
        "                                 y_i + (r/d + arrow_p1) * dy + arrow_p2 * dx,\n",
        "                                 (1 - 2 * r/d - arrow_p3) * dx,\n",
        "                                 (1 - 2 * r/d - arrow_p3) * dy,\n",
        "                                 color=(1 - hmm.A[i][j], ) * 3,\n",
        "                                 head_width=arrow_size, head_length=arrow_size,\n",
        "                                 zorder=int(hmm.A[i][j] * zorder_mult))\n",
        "            else:\n",
        "                arrow = ax.arrow(x_i, y_i, 0, 0,\n",
        "                                 color=(1 - hmm.A[i][j], ) * 3,\n",
        "                                 head_width=arrow_size, head_length=arrow_size,\n",
        "                                 zorder=int(hmm.A[i][j] * zorder_mult))\n",
        "\n",
        "            row.append(arrow)\n",
        "        arrows.append(row)\n",
        "\n",
        "    emission, states = hmm.generate_emission(M, seed=seed)\n",
        "\n",
        "    def animate(i):\n",
        "        if i >= delay:\n",
        "            i -= delay\n",
        "\n",
        "            if i == 0:\n",
        "                arrows[states[0]][states[0]].set_color('red')\n",
        "            elif i == 1:\n",
        "                arrows[states[0]][states[0]].set_color((1 - hmm.A[states[0]][states[0]], ) * 3)\n",
        "                arrows[states[i - 1]][states[i]].set_color('red')\n",
        "            else:\n",
        "                arrows[states[i - 2]][states[i - 1]].set_color((1 - hmm.A[states[i - 2]][states[i - 1]], ) * 3)\n",
        "                arrows[states[i - 1]][states[i]].set_color('red')\n",
        "\n",
        "            # Set text.\n",
        "            text.set_text(' '.join([obs_map_r[e] for e in emission][:i+1]).capitalize())\n",
        "\n",
        "            return arrows + [text]\n",
        "\n",
        "    # Animate!\n",
        "    print('\\nAnimating...')\n",
        "    anim = FuncAnimation(fig, animate, frames=M+delay, interval=1000)\n",
        "\n",
        "    return anim"
      ],
      "metadata": {
        "id": "fBF9IZ4fbthM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0ka2izuNbvXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_visualize = unsupervised_HMM(shakespeare_obs, 10, 100)"
      ],
      "metadata": {
        "id": "oSTZGAUebwfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordclouds = states_to_wordclouds_modified(hmm_visualize, shakespeare_obs_map)"
      ],
      "metadata": {
        "id": "OwS1CJXnbxhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the top ten words in each state\n",
        "def states_to_top_words(hmm, obs_map, max_words=50, show=True):\n",
        "    # Initialize.\n",
        "    M = 100000\n",
        "    n_states = len(hmm.A)\n",
        "    our_states = range(n_states)\n",
        "    obs_map_r = obs_map_reverser(obs_map)\n",
        "\n",
        "    # Generate a large emission.\n",
        "    emission, states = hmm.generate_emission(M)\n",
        "\n",
        "    # For each state, get a list of observations that have been emitted\n",
        "    # from that state.\n",
        "    obs_count = []\n",
        "    for i in range(n_states):\n",
        "        obs_lst = np.array(emission)[np.where(np.array(states) == i)[0]]\n",
        "        obs_count.append(obs_lst)\n",
        "\n",
        "    # For each state, get the frequency of each word and choose the top ten words\n",
        "    for i in range(n_states):\n",
        "        if (i in our_states):\n",
        "            print(f\"HIDDEN STATE {i}\")\n",
        "            state_obs = obs_count[i]\n",
        "            word_freq = {}\n",
        "            for obs in state_obs:\n",
        "                word_freq[obs] = word_freq.get(obs, 0) + 1\n",
        "\n",
        "            top_words = sorted(word_freq, key=word_freq.get, reverse=True)[:10]\n",
        "            top_strs = []\n",
        "            for num in top_words:\n",
        "                top_strs.append(obs_map_r[num])\n",
        "            print(top_strs)\n"
      ],
      "metadata": {
        "id": "KtXm_EDtbzKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states_to_top_words(hmm_visualize, shakespeare_obs_map)"
      ],
      "metadata": {
        "id": "1k1CbKs8bzkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the transitions"
      ],
      "metadata": {
        "id": "FhCf5b5Ib0uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "anim = animate_emission(hmm_visualize, shakespeare_obs_map, M=8, seed=123)\n",
        "HTML(anim.to_html5_video())"
      ],
      "metadata": {
        "id": "mhXcUf_-b2l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKNL9nB5yjDl"
      },
      "source": [
        "## RNN Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA6_nauRkRWm"
      },
      "source": [
        "###Preprocessing for RNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = sorted(list(set(shakespeare_cleaned)))\n",
        "char_to_index = {}\n",
        "for index, c in enumerate(unique_chars):\n",
        "    char_to_index[c] = index"
      ],
      "metadata": {
        "id": "OpVTznoiNcfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh-cAxZtkTLR"
      },
      "outputs": [],
      "source": [
        "sequence_length = 40\n",
        "step = 10  #hyperparameter that we can change\n",
        "sequences = []\n",
        "input = []\n",
        "output = []\n",
        "\n",
        "def processing(shakespeare_cleaned, sequence_length, char_to_index):\n",
        "  for i in range(0, len(shakespeare_cleaned) - sequence_length, step):\n",
        "        input.append(shakespeare_cleaned[i: i + sequence_length - 1])\n",
        "        output.append(shakespeare_cleaned[i + 1: i + sequence_length])\n",
        "        sequences.append(shakespeare_cleaned[i: i + sequence_length])\n",
        "\n",
        "  rnn_tensor= np.zeros((len(sequences), sequence_length , len(unique_chars)))\n",
        "  for i, sequence in enumerate(input):\n",
        "      for j, c in enumerate(sequence):\n",
        "          rnn_tensor[i, j, char_to_index[c]] = 1\n",
        "\n",
        "  rnn_tensor_input = np.zeros((len(input), sequence_length - 1 , len(unique_chars)))\n",
        "  for i, sequence in enumerate(input):\n",
        "      for j, c in enumerate(sequence):\n",
        "          rnn_tensor_input[i, j, char_to_index[c]] = 1\n",
        "\n",
        "  rnn_tensor_output = np.zeros((len(output), sequence_length - 1, len(unique_chars)))\n",
        "  for i, sequence in enumerate(output):\n",
        "      for j, c in enumerate(sequence):\n",
        "          rnn_tensor_output[i, j, char_to_index[c]] = 1\n",
        "  return rnn_tensor, rnn_tensor_input, rnn_tensor_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining and Training RNN Model\n"
      ],
      "metadata": {
        "id": "f79YfFRjPPdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, seq, input, output):\n",
        "        self.data = torch.tensor(seq, dtype=torch.float32)\n",
        "        self.X = torch.tensor(input, dtype=torch.float32)\n",
        "        self.y = torch.tensor(output, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp = self.X[idx]\n",
        "        target = self.y[idx]\n",
        "        return inp, target"
      ],
      "metadata": {
        "id": "mx26BEytNjOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that we pre-processed correctly\n",
        "seq, x, y = processing(shakespeare_cleaned, sequence_length, char_to_index)\n",
        "print(np.shape(seq))\n",
        "print(np.shape(x))\n",
        "print(np.shape(y))\n",
        "train_dataset = MyDataset(seq, x, y)\n",
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "hCDvUxcHNnL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.i2h(input, hidden)\n",
        "        output = self.dropout(output)\n",
        "        output = output.reshape(-1, self.hidden_size)\n",
        "        output = self.h2o(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "      return (torch.zeros(1, batch_size, self.hidden_size),torch.zeros(1, batch_size, self.hidden_size))"
      ],
      "metadata": {
        "id": "WdTrr39KOgbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages to let us see how fast the model trains\n",
        "import time\n",
        "import math\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "metadata": {
        "id": "Kk-kOEZgOnuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1\n",
        "rnn = RNN(len(unique_chars), 128, len(unique_chars))\n",
        "\n",
        "n_iters = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0005\n",
        "start = time.time()\n",
        "rnn.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "\n",
        "    for input, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = input.size(0)\n",
        "        hidden = rnn.initHidden(batch_size)\n",
        "\n",
        "        output, hidden = rnn(input, hidden)\n",
        "        hidden = tuple(h.detach() for h in hidden)\n",
        "        _, target_indices = target.max(dim=-1)\n",
        "        target_indices = target_indices.view(-1)\n",
        "        loss = criterion(output, target_indices)\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "\n",
        "    if iter % 10 == 0 or iter == 0:\n",
        "      print(f'Iteration: {iter}, Loss: {loss}')\n",
        "      print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n"
      ],
      "metadata": {
        "id": "zRAE50oOOqJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Emissions from the RNN Model\n"
      ],
      "metadata": {
        "id": "_G_3ZSvPPU6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, input_sequence, T):\n",
        "  # turn array of characters into tensor of one-hot vectors\n",
        "  tensor_input = np.zeros((1, len(input_sequence) , len(unique_chars)))\n",
        "  for j, c in enumerate(input_sequence):\n",
        "        tensor_input[0, j, char_to_index[c]] = 1\n",
        "  test = torch.tensor(tensor_input, dtype=torch.float32)\n",
        "  hidden = rnn.initHidden(batch_size=1)\n",
        "  model.eval()\n",
        "  # get model predictions\n",
        "  out, hidden = model(test, hidden)\n",
        "  # divide probability distrubtion for the last character of the output by T.\n",
        "  # Then apply softmax\n",
        "  probabilites = torch.softmax(out[-1,:]/T, dim=0)\n",
        "  # Choose character with highest probability\n",
        "  predicted_class = torch.argmax(probabilites, dim=0)\n",
        "  return unique_chars[predicted_class]\n",
        "\n",
        "\n",
        "\n",
        "def sample(model, M, input_seq, window, T):\n",
        "  # Turn string into array of characters\n",
        "  character_list = [c for c in input_seq]\n",
        "  for i in range(M-len(character_list)):\n",
        "    # Look at a fixed amount of previous characters using window to reduce\n",
        "    # repitition in emission\n",
        "    character_list.append(predict(model, character_list[-window:], T))\n",
        "\n",
        "  return ''.join(character_list)\n",
        "\n",
        "for T in [0.25, 0.75, 1, 1.5]:\n",
        "  print(\"Temperature = \" + str(T) + \"\\n\")\n",
        "  print(sample(rnn, 600, \"weary with toil i haste me to my bed\", 43, T))"
      ],
      "metadata": {
        "id": "-TDATIigPZex"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bH_uldS-yjDh",
        "5U5Lb0xqyjDk",
        "QhSIpB8COdqq",
        "zILBESO8v7gk",
        "ctM0uGcwmmiY",
        "kKNL9nB5yjDl",
        "iA6_nauRkRWm",
        "f79YfFRjPPdz",
        "_G_3ZSvPPU6p"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
